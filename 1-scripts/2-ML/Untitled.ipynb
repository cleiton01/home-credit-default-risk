{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    data_set is the set that will be used, only with the column used on the model.\n",
    "    targe is the value that we try to predict. \n",
    "\"\"\"\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data_set, target, test_size=0.4, random_state=0)\n",
    "\n",
    "class Regression():\n",
    "\n",
    "    def get_kernels(key):\n",
    "        kernels = {1:'linear', 2:'poly', 3:'rbf', 4:'sigmoid'}\n",
    "\n",
    "        return kernels.get(key)\n",
    "\n",
    "    def get_loss(key):\n",
    "        loss = {1:'squared_loss', 2:'huber', 3:'epsilon_insensitive', 4:'squared_epsilon_insensitive'}\n",
    "\n",
    "        return loss.get(key)\n",
    "\n",
    "    def get_penalty(key):\n",
    "        penalty={1:None, 2:'l2', 3:'l1', 4:'elasticnet'}\n",
    "\n",
    "        return penalty.get(key)\n",
    "\n",
    "    def get_weights(key):\n",
    "        weights = {1:'uniform', 2:'distance'}\n",
    "        \n",
    "        return weights.get(key)\n",
    "\n",
    "    def algorithm_KRN(key):\n",
    "\n",
    "        algorithm = {1:'ball_tree', 2:'kd_tree', 3:'brute'}\n",
    "\n",
    "        return algorithm.get(key)\n",
    "\n",
    "\n",
    "    def Power_parameter(key):\n",
    "\n",
    "        p = {1:1, 2:2}\n",
    "\n",
    "        return p.get(key)\n",
    "\n",
    "    def get_criterion(key):\n",
    "\n",
    "        criterion = {1:'mse',2:'friedman_mse', 3:'mae'}\n",
    "\n",
    "        return criterion.get(key)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Support Vector Machines\n",
    "\n",
    "        gamma usar valores baixos 0.xxxx\n",
    "        degree apenas para o kernel poly eh um valor inteiro\n",
    "        \"\"\"\n",
    "    def svr(self, kernel='linear', epsilon=0.1, degree=3, gamma='auto', cache_size=50):\n",
    "\n",
    "        model = svm.SVR(kernel=kernel, C=1, epsilon=epsilon, degree=degree, gamma=gamma, cache_size=cache_size)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \"\"\"\n",
    "    def SGDRegressor(loss='squared_loss', penalty='l2', epoch=10):\n",
    "\n",
    "        model = linear_model.SGDRegressor(loss=loss, penalty=penalty, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=epoch, tol=None, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, warm_start=False, average=False, n_iter=None)\n",
    "        \"\"\"\n",
    "            y = np.random.randn(n_samples)\n",
    "            X = np.random.randn(n_samples, n_features)\n",
    "            clf = linear_model.SGDRegressor()\n",
    "            clf.fit(X, y)\n",
    "        \"\"\"\n",
    "        return model\n",
    "\n",
    "    \"\"\"\n",
    "        Nearest Neighbors\n",
    "    \"\"\"\n",
    "    def KNeighborsRegressor(neighbors=5, weights='distance', algorithm='auto', p=2):\n",
    "\n",
    "        model = KNeighborsRegressor(n_neighbors=neighbors, weights=weights, algorithm=algorithm, leaf_size=30, p=p, metric='minkowski', metric_params=None, n_jobs=1)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \"\"\"\n",
    "        Nearest Neighbors\n",
    "    \"\"\"\n",
    "    def RadiusNeighborsRegressor(radius=1.0, weights='distance', algorithm='auto', p=2):\n",
    "\n",
    "        model = RadiusNeighborsRegressor(radius=radius, weights=weights, algorithm=algorithm, leaf_size=30, p=p, metric='minkowski', metric_params=None)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \"\"\"\n",
    "        Decision Trees\n",
    "    \"\"\"\n",
    "    def DecisionTreeRegressor(criterion='mse', splitter='best'):\n",
    "\n",
    "        model = tree.DecisionTreeRegressor(criterion=criterion, splitter=splitter, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, presort=False)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def loss_GRADIENT(key):\n",
    "        loss = {1:'ls', 2:'lad', 3:'huber', 4:'quantile'}\n",
    "\n",
    "        return loss.get(key)\n",
    "    \n",
    "    \"\"\"\n",
    "        Ensemble methods\n",
    "    \"\"\"\n",
    "\n",
    "    def GradientBoostingRegressor(loss=0.1,learning_rate=0.1, n_estimators=100, max_depth=3, criterion='friedman_mse'):\n",
    "\n",
    "        model = GradientBoostingRegressor(loss=loss, learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth, criterion=criterion)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_loss_ADAB(key):\n",
    "        \n",
    "        loss = {'linear', 'square', 'exponential'}\n",
    "\n",
    "        return loss.get(key)\n",
    "\n",
    "    def AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None):\n",
    "\n",
    "        model = AdaBoostRegressor(base_estimator=base_estimator, n_estimators=n_estimators, learning_rate=learning_rate, loss=loss, random_state=random_state)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def LogisticRegression(random_state=1):\n",
    "\n",
    "        model = LogisticRegression(random_state=random_state)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \"\"\"\n",
    "        Multiclass and multilabel algorithms\n",
    "    \"\"\"\n",
    "\n",
    "    def MultiOutputRegressor(model):\n",
    "        from sklearn.multioutput import MultiOutputRegressor\n",
    "        mult_model = MultiOutputRegressor(model)\n",
    "        mult_model.fit(X,Y).predict(x_predict)\n",
    "\n",
    "    \"\"\"\n",
    "        Multiclass and multilabel algorithms\n",
    "    \"\"\"\n",
    "    def MultiOutputClassifier(model):\n",
    "        from sklearn.multioutput import MultiOutputClassifier\n",
    "        mult_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "        mult_model.fit(X, Y).predict(x_predict)\n",
    "\n",
    "    \"\"\"\n",
    "        Neural network models (supervised)\n",
    "    \"\"\"\n",
    "    def MLPRegressor(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200):\n",
    "\n",
    "        model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, alpha=alpha, batch_size=batch_size, learning_rate=learning_rate, learning_rate_init=learning_rate_init, power_t=power_t, max_iter=max_iter)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def Sequential(dim=(10, 10, 10, 1), kernel_initializer='normal', activation='relu', loss='mean_square_error', optimizer='adam'):\n",
    "        model = Sequential()\n",
    "        model.add(Danse(dim[0], input_dim=dim[0], kernel_initializer=kernel_initializer, activation=activation))\n",
    "        model.add(Dense(dim[1], kernel_initializer=kernel_initializer, activation=activation))\n",
    "        model.add(Dense(dim[2], kernel_initializer=kernel_initializer, activation=activation))\n",
    "        model.add(Dense(dim[3], kernel_initializer=kernel_initializer))\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "        return model\n",
    "    \"\"\"\n",
    "    Ensemble methods\n",
    "        GradientBoostingRegressor\n",
    "        AdaBoostRegressor\n",
    "    Neural network models (supervised)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcdr-project",
   "language": "python",
   "name": "hcdr-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
